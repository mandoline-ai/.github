# Mandoline

[Mandoline](https://mandoline.ai) helps developers evaluate and improve LLM applications in ways that matter to users.

Create custom metrics that align with your specific use case, evaluate LLM performance in real situations, and track improvements over time.

## Documentation

- [Getting Started](https://mandoline.ai/docs/getting-started-with-mandoline)
- [Core Concepts](https://mandoline.ai/docs/mandoline-core-concepts)
- [Best LLM For](https://mandoline.ai/docs/best-llm-for)
- [API Reference](https://mandoline.ai/docs/mandoline-api-reference)

## Tutorials

- [Multimodal Evaluation: Evaluate LLMs Across Text and Vision Tasks](https://mandoline.ai/docs/tutorials/multimodal-evaluation-text-and-vision-tasks)
- [Model Selection: Comparing LLMs for Creative Tasks](https://mandoline.ai/docs/tutorials/model-selection-compare-llms-for-creative-tasks)
- [Prompt Engineering: Reduce Unwanted LLM Behaviors](https://mandoline.ai/docs/tutorials/prompt-engineering-reduce-unwanted-llm-behaviors)

## Analysis & Insights

- [Multimodal Language Model Evaluation: A Creative Coding Challenge](https://mandoline.ai/blog/multimodal-evals-creative-coding)
- [Refusal Rates in Open-Source vs. Proprietary Language Models](https://mandoline.ai/blog/open-source-vs-proprietary-llm-refusals)
- [Comparing Refusal Behavior Across Top Language Models](https://mandoline.ai/blog/comparing-llm-refusal-behavior)

## Leaderboards

- [Refusals](https://mandoline.ai/leaderboards/refusals)

## SDKs

- [Node.js](https://github.com/mandoline-ai/mandoline-node)
- [Python](https://github.com/mandoline-ai/mandoline-python)

## Support

- [GitHub Issues](https://github.com/mandoline-ai/mandoline-node/issues)
- [Email](mailto:support@mandoline.ai)
